{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for data preprocessing\n",
    "def data_preprocessing(normalize,reshape):\n",
    "    # Importing data set CIFAR-10\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "    K = len(np.unique(y_train)) # no.of Classes \n",
    "    Ntr = x_train.shape[0] # No.of training data=50,000\n",
    "    Nte = x_test.shape[0] # No.of testing data=10,000\n",
    "    Din = 3072 # CIFAR10 = 3072 = 32x32x3\n",
    "\n",
    "    if normalize:\n",
    "        x_train, x_test = x_train / 255.0, x_test / 255.0 # Normalize pixel values\n",
    "\n",
    "    # Center the pixel values\n",
    "    mean_image = np.mean(x_train, axis=0)\n",
    "    x_train = x_train - mean_image\n",
    "    x_test = x_test - mean_image\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "    #flatterning the input images\n",
    "    if reshape:\n",
    "        x_train = np.reshape(x_train,(Ntr,Din))\n",
    "        x_test = np.reshape(x_test,(Nte,Din))\n",
    "    # Changing the data types\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    return x_train,y_train,x_test,y_test,K,Din,Ntr,Nte\n",
    "\n",
    "\n",
    "# Defining linear Classifier function\n",
    "def l1_linear_classifier(x_train,y_train,x_test,y_test,K,Din,lr,lr_decay,reg,Ntr,Nte):\n",
    "    batch_size = Ntr\n",
    "    loss_history = []\n",
    "    loss_history_testing = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    lr_array=[]\n",
    "    seed = 0\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # Initializing weight and bias arrays\n",
    "    std=1e-5 #standard deviation to generate random values for w1 and b1\n",
    "    w1 = std*np.random.randn(Din, K) #weight matrix\n",
    "    b1 = np.zeros(K) #k dimensional bias matrix\n",
    "\n",
    "    for t in range(iterations):\n",
    "        # To prevent over fitting we shuffle the training data set in order to randomize the training process.\n",
    "        indices = np.arange(Ntr)\n",
    "        rng.shuffle(indices)\n",
    "        x=x_train[indices]\n",
    "        y=y_train[indices]\n",
    "\n",
    "        # forward propagation\n",
    "        y_pred=x.dot(w1)+b1\n",
    "        y_pred_test=x_test.dot(w1)+b1\n",
    "        val=y_pred_test.shape[0]\n",
    "        # calculating loss using regularized loss function\n",
    "        train_loss=(1/batch_size)*(np.square(y_pred-y)).sum()+reg*(np.sum(w1*w1))\n",
    "        loss_history.append(train_loss)\n",
    "        test_loss=(1/val)*(np.square(y_pred_test-y_test)).sum()+reg*(np.sum(w1*w1))\n",
    "        loss_history_testing.append(test_loss)\n",
    "        \n",
    "        # calculating trainning and testing accuracies\n",
    "        train_accuracy=1-(1/(10*batch_size))*(np.abs(np.argmax(y,axis=1)-np.argmax(y_pred,axis=1))).sum()\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        test_accuracy=1-(1/(10*Nte))*(np.abs(np.argmax(y_test,axis=1)-np.argmax(y_pred_test,axis=1))).sum()\n",
    "        val_acc_history.append(test_accuracy)\n",
    "\n",
    "        if t%10 == 0:\n",
    "            print('epoch %d/%d: train loss= %f || ,test loss= %f ||,train accuracy= %f ||, test accuracy= %f ||, learning rate= %f ||' \n",
    "            % (t,iterations,train_loss,test_loss,train_accuracy,test_accuracy,lr))\n",
    "\n",
    "        # Backward propagation\n",
    "        dy_pred=(1./batch_size)*2.0*(y_pred-y)#partial deravative of L w.r.t y_pred\n",
    "        dw1=x.T.dot(dy_pred)+reg*w1\n",
    "        db1=dy_pred.sum(axis=0)\n",
    "\n",
    "        \n",
    "        w1-=lr*dw1#update weight matrix\n",
    "        b1-=lr*db1#update bias matrix\n",
    "        lr_array.append(lr)\n",
    "        lr*=lr_decay#decaying the learning rate\n",
    "    return w1,b1,loss_history,loss_history_testing,train_acc_history,val_acc_history,lr_array\n",
    "\n",
    "\n",
    "#defining parameters \n",
    "iterations = 300#gradient descent iterations\n",
    "lr = 1.4e-2#learning rate\n",
    "lr_decay= 0.999\n",
    "reg = 5e-6 #lambda=regularization parameter\n",
    "x_train,y_train,x_test,y_test,K,Din,Ntr,Nte=data_preprocessing(normalize=True,reshape=True)\n",
    " #Run the linear classifier\n",
    "w1,b1,loss_history,loss_history_test,train_acc_history,val_acc_history,lr_array=l1_linear_classifier(x_train,y_train,x_test,y_test,K,Din,lr,lr_decay,reg,Ntr,Nte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the graphs of training and testing losses, training and testing accuracies and learning rate\n",
    "fig, axes  = plt.subplots(1,5)\n",
    "titles = {\"Training Loss\":loss_history, \"testing loss\":loss_history_test,\"Training Accuracy\":train_acc_history,\n",
    "         \"testing Accuracy\": val_acc_history, \"Learning Rate\":lr_array}\n",
    "place = 0\n",
    "for key in titles.keys():\n",
    "    axes[place].plot(titles[key])\n",
    "    axes[place].set_title(key)\n",
    "    place+=1\n",
    "plt.show()\n",
    "#plotting the weight matrix W as 10 images\n",
    "images=[]\n",
    "classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']#classes of cifar-10\n",
    "for i in range(w1.shape[1]):\n",
    "    reshapen=np.reshape(w1[:,i]*255,(32,32,3))\n",
    "    normalized=cv.normalize(reshapen, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)\n",
    "    images.append(normalized)\n",
    "fig,ax=plt.subplots(2,5,figsize=(30,10))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        ax[i,j].imshow(images[i*5+j],vmin=0,vmax=255)\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        ax[i,j].set_title(classes[i*5+j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2\n",
    "# Function for two layer dense network\n",
    "def layer_2(x_train,y_train,x_test,y_test,Din,lr,lr_decay,H,reg,K,Ntr,Nte):\n",
    "    loss_history = []\n",
    "    loss_history_test = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    lr_array =[]\n",
    "    seed = 0\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    batch_size=Ntr\n",
    "\n",
    "    std=1e-5\n",
    "    #initializing weight and bias matrices for hidden layer\n",
    "    w1 = std*np.random.randn(Din, H)\n",
    "    b1 = np.zeros(H)\n",
    "    #initializing weight and bias matrices for final layer\n",
    "    w2 = std*np.random.randn(H, K)\n",
    "    b2 = np.zeros(K)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        indices = np.arange(Ntr)\n",
    "        \n",
    "        rng.shuffle(indices)# To avoid overfitting shuffle the training data set\n",
    "        x=x_train[indices]\n",
    "        y=y_train[indices]\n",
    "\n",
    "        #forward propagation\n",
    "        h=1/(1+np.exp(-(x.dot(w1)+b1)))\n",
    "        h_test=1/(1+np.exp(-((x_test).dot(w1)+b1)))\n",
    "        y_pred=h.dot(w2)+b2\n",
    "        y_pred_test=h_test.dot(w2)+b2\n",
    "        val=y_pred_test.shape[0]\n",
    "        # calculating the training and testing loss\n",
    "        training_loss=(1/batch_size)*(np.square(y_pred-y)).sum()+reg*(np.sum(w1*w1)+np.sum(w2*w2))\n",
    "        loss_history.append(training_loss)\n",
    "        testing_loss=(1/val)*(np.square(y_pred_test-y_test)).sum()+reg*(np.sum(w1*w1)+np.sum(w2*w2))\n",
    "        loss_history_test.append(testing_loss)\n",
    "        \n",
    "        # calculating trainning and testing accuracies\n",
    "        train_accuracy=1-(1/(10*batch_size))*(np.abs(np.argmax(y,axis=1)-np.argmax(y_pred,axis=1))).sum()\n",
    "        train_acc_history.append(train_accuracy)\n",
    "\n",
    "        test_accuracy=1-(1/(10*Nte))*(np.abs(np.argmax(y_test,axis=1)-np.argmax(y_pred_test,axis=1))).sum()\n",
    "        val_acc_history.append(test_accuracy)\n",
    "        # Print for every 10 iterations\n",
    "        if i%10 == 0:\n",
    "            print('epoch %d/%d: loss= %f || , test loss= %f ||, train accuracy= %f ||, test accuracy= %f ||, learning rate= %f ||' \n",
    "            % (i,iterations,training_loss,testing_loss,train_accuracy,test_accuracy,lr))\n",
    "\n",
    "        # Backward propagation\n",
    "        #let's find the deravatives of the learnable parameters\n",
    "        dy_pred=(1./batch_size)*2.0*(y_pred-y)#partial deravative of L w.r.t y_pred\n",
    "        dw2=h.T.dot(dy_pred)+reg*w2\n",
    "        db2=dy_pred.sum(axis=0)\n",
    "        dh=dy_pred.dot(w2.T)\n",
    "        dw1=x.T.dot(dh*h*(1-h))+reg*w1\n",
    "        db1=(dh*h*(1-h)).sum(axis=0)\n",
    "        #update weight matrices\n",
    "        w1-=lr*dw1 \n",
    "        w2-=lr*dw2\n",
    "        #update bias matrices\n",
    "        b1-=lr*db1\n",
    "        b2-=lr*db2\n",
    "        lr_array.append(lr)\n",
    "        lr*=lr_decay#decaying the learning rate\n",
    "    return w1,b1,w2,b2,loss_history,loss_history_test,train_acc_history,val_acc_history,lr_array\n",
    "x_train_2layer,y_train_2layer,x_test_2layer,y_test_2layer,K,Din,Ntr,Nte=preprocessing(normalize=False,reshape=True)\n",
    "#Remove the normalization. Otherwise the model will not learn \n",
    "iterations = 300#gradient descent iterations\n",
    "lr = 1.4e-2#learning rate\n",
    "lr_decay= 0.999\n",
    "reg = 5e-6#lambda=regularization parameter\n",
    "H=200 #hidden layer nodes  \n",
    "w1_2layer,b1_2layer,w2_2layer,b2_2layer,loss_history_2layer,loss_history_test_2layer,train_acc_history_2layer,val_acc_history_2layer,lr_array_2layer=layer_2(x_train_2layer,y_train_2layer,x_test_2layer,y_test_2layer,Din,lr,lr_decay,H,reg,K,Ntr,Nte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the graphs of training and testing losses, training and testing accuracies and learning rate\n",
    "fig, axes  = plt.subplots(1,5,figsize=(50,10))\n",
    "titles = {\"Training Loss\":loss_history_2layer, \"testing loss\":loss_history_test_2layer,\"Training Accuracy\":train_acc_history_2layer,\n",
    "         \"testing Accuracy\": val_acc_history_2layer, \"Learning Rate\":lr_array_2layer}\n",
    "place = 0\n",
    "for key in titles.keys():\n",
    "    axes[place].plot(titles[key])\n",
    "    axes[place].set_title(key)\n",
    "    place+=1\n",
    "plt.show()\n",
    "#plotting the weight matrix W as 10 images\n",
    "images=[]\n",
    "classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']#classes of cifar-10\n",
    "for i in range(w1_2layer.shape[1]):\n",
    "    reshapen=np.reshape(w1_2layer[:,i]*255,(32,32,3))\n",
    "    normalized=cv.normalize(reshapen, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)\n",
    "    images.append(normalized)\n",
    "fig,ax=plt.subplots(2,5,figsize=(40,10))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        ax[i,j].imshow(images[i*5+j],vmin=0,vmax=255)\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        ax[i,j].set_title(classes[i*5+j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 3\n",
    "\n",
    "# Function for two layer dense network with stochastic gradient descent\n",
    "def batching(x_train,y_train,x_test,y_test,Din,lr,lr_decay,H,reg,K,Ntr,Nte):\n",
    "    loss_history = []\n",
    "    loss_history_test = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    lr_array =[]\n",
    "    seed = 0\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    batch_size=500 #make batch size =500 for stochastic gradient descent\n",
    "\n",
    "    std=1e-5\n",
    "    #initializing weight and bias matrices for hidden layer\n",
    "    w1 = std*np.random.randn(Din, H)\n",
    "    b1 = np.zeros(H)\n",
    "    #initializing weight and bias matrices for final layer\n",
    "    w2 = std*np.random.randn(H, K)\n",
    "    b2 = np.zeros(K)\n",
    "\n",
    "    for j in range(iterations):\n",
    "        training_loss = 0\n",
    "        testing_loss=0\n",
    "        train_accuracy=0\n",
    "        test_accuracy=0\n",
    "        for begin in range(0,Ntr,batch_size):#running 100 groups for each epoch \n",
    "            indices = np.arange(Ntr)\n",
    "            indices=indices[begin:begin+batch_size]#taking only 500 samples\n",
    "            rng.shuffle(indices)j\n",
    "            x=x_train[indices]\n",
    "            y=y_train[indices]\n",
    "            #forward propagation\n",
    "            h=1/(1+np.exp(-(x.dot(w1)+b1)))\n",
    "            y_pred=h.dot(w2)+b2\n",
    "            h_test=1/(1+np.exp(-((x_test).dot(w1)+b1)))\n",
    "            y_pred_test=h_test.dot(w2)+b2\n",
    "            val=y_pred_test.shape[0]\n",
    "            # calculating the training and testing loss for each mini batch\n",
    "            mini_training_loss=(1/batch_size)*(np.square(y_pred-y)).sum()+reg*(np.sum(w1*w1)+np.sum(w2*w2))\n",
    "            mini_testing_loss=(1/val)*(np.square(y_pred_test-y_test)).sum()+reg*(np.sum(w1*w1)+np.sum(w2*w2))\n",
    "            training_loss+= mini_training_loss\n",
    "            testing_loss+= mini_testing_loss\n",
    "\n",
    "            # calculating trainning and testing accuracies for each mini batch\n",
    "            mini_train_accuracy=1-(1/(10*batch_size))*(np.abs(np.argmax(y,axis=1)-np.argmax(y_pred,axis=1))).sum()\n",
    "            mini_test_accuracy=1-(1/(10*Nte))*(np.abs(np.argmax(y_test,axis=1)-np.argmax(y_pred_test,axis=1))).sum()\n",
    "            train_accuracy+=mini_train_accuracy\n",
    "            test_accuracy+=mini_test_accuracy\n",
    "            \n",
    "            # Backward propagation\n",
    "            #let's find the deravatives of the learnable parameters\n",
    "            dy_pred=(1./batch_size)*2.0*(y_pred-y)\n",
    "            dw2=h.T.dot(dy_pred)+reg*w2\n",
    "            db2=dy_pred.sum(axis=0)\n",
    "            dh=dy_pred.dot(w2.T)\n",
    "            dw1=x.T.dot(dh*h*(1-h))+reg*w1\n",
    "            db1=(dh*h*(1-h)).sum(axis=0)\n",
    "            #update weight matrices\n",
    "            w1-=lr*dw1 \n",
    "            w2-=lr*dw2\n",
    "            #update bias matrices\n",
    "            b1-=lr*db1\n",
    "            b2-=lr*db2\n",
    "        #taking average of 100 groups to find\n",
    "        train_accuracy=train_accuracy/(Ntr/batch_size)\n",
    "        test_accuracy=(test_accuracy/(Ntr/batch_size))\n",
    "        #taking average of 100 groups to find loss\n",
    "        training_loss=training_loss/(Ntr/batch_size)\n",
    "        testing_loss=testing_loss/(Ntr/batch_size)\n",
    "\n",
    "        loss_history.append(training_loss)\n",
    "        loss_history_test.append(testing_loss)\n",
    "        train_acc_history.append(train_accuracy)\n",
    "        val_acc_history.append(test_accuracy)\n",
    "        lr_array.append(lr)\n",
    "        lr*=lr_decay#decaying the learning rate\n",
    "        if j%1 == 0:\n",
    "                print('epoch %d/%d: loss= %f || , test loss= %f ||, train accuracy= %f ||, test accuracy= %f ||, learning rate= %f ||'  \n",
    "                % (j,iterations,training_loss,testing_loss,train_accuracy,test_accuracy,lr))\n",
    "    return w1,b1,w2,b2,loss_history,loss_history_test,train_acc_history,val_acc_history,lr_array\n",
    "\n",
    "\n",
    "batch_size = 500\n",
    "H=200#hidden layer nodes  \n",
    "iterations = 300#gradient descent iterations\n",
    "lr = 1.4e-2\n",
    "lr_decay= 0.999\n",
    "reg = 5e-6#lambda=regularization parameter\n",
    "x_train_2layer,y_train_2layer,x_test_2layer,y_test_2layer,K,Din,Ntr,Nte=data_preprocessing(normalize=False,reshape=True)\n",
    "#Remove the normalization. Otherwise the model will not learn \n",
    "w1_batching,b1_batching,w2_batching,b2_batching,loss_history_batching,loss_history_test_batching,train_acc_history_batching,val_acc_history_batching,lr_array_batching=batching(x_train_2layer,y_train_2layer,x_test_2layer,y_test_2layer,Din,lr,lr_decay,H,reg,K,Ntr,Nte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting the graphs of training and testing losses, training and testing accuracies and learning rate\n",
    "fig, axes  = plt.subplots(1,5,figsize=(50,10))\n",
    "titles = {\"Training Loss\":[loss_history_2layer,loss_history_batching], \"testing loss\":[loss_history_test_2layer,loss_history_test_batching],\n",
    "\"Training Accuracy\":[train_acc_history_2layer,train_acc_history_batching], \"testing Accuracy\": [val_acc_history_2layer,val_acc_history_batching], \n",
    "\"Learning Rate\":[lr_array_2layer,lr_array_batching]}\n",
    "place = 0\n",
    "for key in titles.keys():\n",
    "    if place==0:\n",
    "        axes[place].plot(titles[key][0],label='with gradient descent')\n",
    "        axes[place].plot(titles[key][1],label='with stochastic gradient descent')\n",
    "        axes[place].legend()\n",
    "    else:\n",
    "        axes[place].plot(titles[key][0])\n",
    "        axes[place].plot(titles[key][1])\n",
    "    axes[place].set_xlabel(\"iteration\")\n",
    "    axes[place].set_title(key)\n",
    "    place+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 4\n",
    "from tensorflow.keras import layers,models,optimizers\n",
    "x_train_CNN,y_train_CNN,x_test_CNN,y_test_CNN,K,Din,Ntr,Nte=data_preprocessing(normalize=True,reshape=False)\n",
    "model = models.Sequential()\n",
    "#layers\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=1.4e-2, momentum=0.9, decay=1e-6)#lr =learning rate, decay =learning rate decay\n",
    "model.compile(optimizer=sgd,loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "history=model.fit(x_train_CNN,y_train_CNN,epochs=50,batch_size=50,validation_data=(x_test_CNN,y_test_CNN))\n",
    "\n",
    "print(model.optimizer.get_config())\n",
    "plt.plot(history.history['loss'],label='training loss')#plotting the graphs\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,50])\n",
    "plt.show()\n",
    "plt.plot(history.history['val_loss'],label='testing loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,50])\n",
    "plt.show()\n",
    "plt.plot(history.history['accuracy'],label='training accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,50])\n",
    "plt.show()\n",
    "plt.plot(history.history['val_accuracy'],label='testing accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([0,50])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
